{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Dataset\n",
    "1. https://github.com/zxaoyou/segmentation_WBC\n",
    "2. https://raabindata.com/free-data/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "import sys\n",
    "\n",
    "import tensorflow as tf\n",
    "from keras import layers, models\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Activation, Conv2D, Flatten, Dropout, MaxPooling2D, BatchNormalization\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "# from PIL import Image\n",
    "import os\n",
    "import csv\n",
    "import numpy as np\n",
    "# import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import shutil\n",
    "import logging\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import skimage.morphology as morph\n",
    "\n",
    "\n",
    "import multiprocessing as mp\n",
    "mp.set_start_method('spawn', force=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No GPU found\n"
     ]
    }
   ],
   "source": [
    "# Configuration\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '-1'\n",
    "force = False # Force the image preprocessing again\n",
    "types = ['Basophil', 'Eosinophil', 'Lymphocyte', 'Monocyte', 'Neutrophil']\n",
    "logging.basicConfig(format='%(levelname)s:%(message)s', level=logging.INFO)\n",
    "\n",
    "raw_train = 'data/raw/train/'\n",
    "raw_test = 'data/raw/test/'\n",
    "\n",
    "train_folder = 'data/train'\n",
    "validation_folder = 'data/validation'\n",
    "test_folder = 'data/test'\n",
    "\n",
    "# percentage of each cell present in each dataset\n",
    "\n",
    "train_size = 0.70\n",
    "validation_size = 0.15\n",
    "test_size = 0.15\n",
    "\n",
    "if tf.test.gpu_device_name():\n",
    "    print('GPU found')\n",
    "    tf.config.experimental.set_virtual_device_configuration(\n",
    "       tf.config.list_physical_devices('GPU')[0],\n",
    "       [\n",
    "           tf.config.experimental.VirtualDeviceConfiguration(memory_limit=8192)\n",
    "        ])\n",
    "    tf.config.experimental.set_memory_growth(tf.config.list_physical_devices('GPU')[0], True)\n",
    "else:\n",
    "    print(\"No GPU found\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def create_generator(folder, subset):\n",
    "    df = pd.read_csv(folder + '/class.csv', dtype=str, delimiter=',')\n",
    "    data = ImageDataGenerator(rescale=1./255.)\n",
    "    return data.flow_from_dataframe(\n",
    "        dataframe=df,\n",
    "        directory=folder,\n",
    "        x_col='Image',\n",
    "        y_col='Id',\n",
    "        subset=subset,\n",
    "        batch_size=128,\n",
    "        shuffle=True,\n",
    "        class_mode='categorical',\n",
    "        target_size=(128, 128),\n",
    "        validation_split=0.25\n",
    "    )\n",
    "\n",
    "def resize(cell, open_file=False):\n",
    "    if open_file:\n",
    "        cell = cv2.imread(cell)\n",
    "    shape = cell.shape\n",
    "    # Cropping the picture\n",
    "    cell = np.delete(cell, np.s_[shape[0] - 150:shape[0]], axis=0)\n",
    "    cell = np.delete(cell, np.s_[0:150], axis=0)\n",
    "    cell = np.delete(cell, np.s_[shape[0] - 150:shape[0]], axis=1)\n",
    "    cell = np.delete(cell, np.s_[0:150], axis=1)\n",
    "\n",
    "    # resizing\n",
    "    cell = cv2.resize(cell, (128, 128), interpolation=cv2.INTER_AREA)\n",
    "    ori = np.copy(cell)\n",
    "    cell = np.dot(cell[...,:3], [0.2999, 0.587, 0.114])\n",
    "    cell = cell > 100\n",
    "    return cell, ori\n",
    "\n",
    "def get_core(cell, ori=None):\n",
    "    if ori is None:\n",
    "        cell, ori = cell\n",
    "    shape = np.uint8(np.invert(morph.remove_small_holes(cell, 1024)))\n",
    "    # plt.imshow(shape, cmap='gray')\n",
    "    zeros = np.zeros((128, 128))\n",
    "    shape = np.stack((zeros + shape, shape, zeros + shape), axis=2)\n",
    "    return np.uint8(ori * shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:Starting program...\n",
      "INFO:Starting WBC classification...\n"
     ]
    }
   ],
   "source": [
    "logging.info(\"Starting program...\")\n",
    "train_csv = None\n",
    "\n",
    "if not os.path.isdir('data/train') or force is True:\n",
    "\n",
    "    logging.warning(\"No train folder detected...\")\n",
    "    logging.info(\"Creating folders...\")\n",
    "\n",
    "\n",
    "    if not os.path.isdir(train_folder):\n",
    "        os.mkdir(train_folder)\n",
    "        os.mkdir(test_folder)\n",
    "\n",
    "    # Load Train\n",
    "    train_csv = open(train_folder + '/class.csv', 'w', newline='')\n",
    "    test_csv = open(test_folder + '/class.csv', 'w', newline='')\n",
    "    train_writer = csv.writer(train_csv)\n",
    "    test_writer = csv.writer(test_csv)\n",
    "    train_writer.writerow(['Image', 'Id'])\n",
    "    test_writer.writerow(['Image', 'Id'])\n",
    "    i = 1\n",
    "    for c in types:\n",
    "        logging.info(c)\n",
    "        folder_size = len([name for name in os.listdir(raw_train + c) if os.path.isfile(name)])\n",
    "        count = 0\n",
    "        for f in os.listdir(raw_train + c):\n",
    "            # if os.path.isfile('data/' + c):\n",
    "            p = get_core(resize(raw_train + c + '/' + f, open_file=True))\n",
    "            cv2.imwrite(train_folder + '/' + f, p)\n",
    "            # shutil.copy(raw_train + c + '/' + f, train_folder + '/' + f)\n",
    "            train_writer.writerow([f, i])\n",
    "            count += 1\n",
    "            # if train_size >= folder_size * train_size:\n",
    "            #     break\n",
    "        count = 0\n",
    "        for f in os.listdir(raw_test + c):\n",
    "            # if os.path.isfile('data/' + c):\n",
    "            p = get_core(resize(raw_test + c + '/' + f, open_file=True))\n",
    "            cv2.imwrite(test_folder + '/' + f, p)\n",
    "            # shutil.copy(raw_train + c + '/' + f, train_folder + '/' + f)\n",
    "            test_writer.writerow([f, i])\n",
    "            # shutil.copy(raw_test + c + '/' + f, test_folder + '/' + f)\n",
    "            # test_writer.writerow([f, i])\n",
    "        i += 1\n",
    "    train_csv.close()\n",
    "    test_csv.close()\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "    with open(dataset_folder + '.csv', 'r') as f:\n",
    "        reader = csv.reader(f)\n",
    "        for line in reader:\n",
    "            classification.append(line[1])\n",
    "    if not os.path.isdir('data/train'):\n",
    "        os.mkdir(train_folder)\n",
    "        os.mkdir(validation_folder)\n",
    "        os.mkdir(test_folder)\n",
    "\n",
    "    i = 1\n",
    "    file = None\n",
    "    writer = None\n",
    "    current = train_folder\n",
    "    for f in os.listdir(dataset_folder):\n",
    "        if i == 1:\n",
    "            logging.info(\"Creating test dataset...\")\n",
    "            file = open(train_folder + '/class.csv', 'w', newline='')\n",
    "            writer = csv.writer(file)\n",
    "            writer.writerow(['Image', 'Id'])\n",
    "        elif i == train_size :\n",
    "            logging.info(\"Creating validation dataset...\")\n",
    "            file.close()\n",
    "            file = open(validation_folder + '/class.csv', 'w', newline='')\n",
    "            current = validation_folder\n",
    "            writer = csv.writer(file)\n",
    "            writer.writerow(['Image', 'Id'])\n",
    "        elif i == train_size + validation_size:\n",
    "            logging.info(\"Creating test dataset...\")\n",
    "            file.close()\n",
    "            file = open(test_folder + '/class.csv', 'w', newline='')\n",
    "            current = test_folder\n",
    "            writer = csv.writer(file)\n",
    "            writer.writerow(['Image', 'Id'])\n",
    "\n",
    "        if os.path.isfile(dataset_folder + '/' + f) and 'bmp' in f:\n",
    "            gray_scale(dataset_folder + '/' + f, current + '/' + f)\n",
    "            writer.writerow([f, classification[i]])\n",
    "            i += 1\n",
    "    file.close()\n",
    "    logging.info(\"Creation complete!\")\n",
    "    print(classification)\n",
    "else:\n",
    "    logging.info(\"Train folder detected!\")\n",
    "\"\"\"\n",
    "logging.info(\"Starting WBC classification...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:Creating generator...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 10175 validated image filenames belonging to 5 classes.\n",
      "Found 0 validated image filenames belonging to 5 classes.\n"
     ]
    }
   ],
   "source": [
    "logging.info(\"Creating generator...\")\n",
    "train_generator = create_generator(train_folder, 'training')\n",
    "# validation_generator = create_generator(validation_folder, 'validation')\n",
    "test_generator = create_generator(test_folder, 'validation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:Training...\n"
     ]
    }
   ],
   "source": [
    "logging.info(\"Training...\")\n",
    "model = models.Sequential()\n",
    "model.add(layers.Conv2D(128, activation='relu', input_shape=(128, 128, 3), kernel_size=(5, 5), strides=1))\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "model.add(layers.Conv2D(256, (5, 5), activation='relu'))\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "model.add(layers.Conv2D(512, (5, 5), activation='relu'))\n",
    "model.add(layers.Dropout(0.25))\n",
    "model.add(Flatten())\n",
    "model.add(layers.Dense(128, activation='relu'))\n",
    "model.add(layers.Dense(5, activation='softmax'))\n",
    "model.compile(optimizer='adam', metrics=['accuracy'], loss=tf.keras.losses.categorical_crossentropy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "80/80 - 730s - loss: 1.2175 - accuracy: 0.7290 - val_loss: 0.4757 - val_accuracy: 0.8370 - 730s/epoch - 9s/step\n",
      "Epoch 2/10\n",
      "80/80 - 721s - loss: 0.3575 - accuracy: 0.8798 - val_loss: 0.2961 - val_accuracy: 0.9017 - 721s/epoch - 9s/step\n",
      "Epoch 3/10\n",
      "80/80 - 722s - loss: 0.3067 - accuracy: 0.8957 - val_loss: 0.2770 - val_accuracy: 0.9036 - 722s/epoch - 9s/step\n",
      "Epoch 4/10\n",
      "80/80 - 726s - loss: 0.2795 - accuracy: 0.9034 - val_loss: 0.2358 - val_accuracy: 0.9206 - 726s/epoch - 9s/step\n",
      "Epoch 5/10\n",
      "80/80 - 801s - loss: 0.2429 - accuracy: 0.9169 - val_loss: 0.2163 - val_accuracy: 0.9265 - 801s/epoch - 10s/step\n",
      "Epoch 6/10\n",
      "80/80 - 721s - loss: 0.2144 - accuracy: 0.9225 - val_loss: 0.1509 - val_accuracy: 0.9497 - 721s/epoch - 9s/step\n",
      "Epoch 7/10\n",
      "80/80 - 730s - loss: 0.1814 - accuracy: 0.9341 - val_loss: 0.1297 - val_accuracy: 0.9572 - 730s/epoch - 9s/step\n",
      "Epoch 8/10\n",
      "80/80 - 740s - loss: 0.1419 - accuracy: 0.9493 - val_loss: 0.0904 - val_accuracy: 0.9701 - 740s/epoch - 9s/step\n",
      "Epoch 9/10\n",
      "80/80 - 725s - loss: 0.1090 - accuracy: 0.9599 - val_loss: 0.0749 - val_accuracy: 0.9758 - 725s/epoch - 9s/step\n",
      "Epoch 10/10\n",
      "80/80 - 741s - loss: 0.0818 - accuracy: 0.9707 - val_loss: 0.0445 - val_accuracy: 0.9884 - 741s/epoch - 9s/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 3 of 3). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:Assets written to: model\\assets\n"
     ]
    }
   ],
   "source": [
    "model.fit(train_generator, validation_data=train_generator, epochs=10, verbose=2)\n",
    "model.save('model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'1': 0, '2': 1, '3': 2, '4': 3, '5': 4}\n"
     ]
    }
   ],
   "source": [
    "print(test_generator.class_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13/80 [===>..........................] - ETA: 2:12 - loss: 0.0372 - accuracy: 0.9886\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "score = model.evaluate(train_generator)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/device:CPU:0', '/device:GPU:0']\n",
      "test\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.python.client import device_lib\n",
    "def get_available_devices():\n",
    "    local_device_protos = device_lib.list_local_devices()\n",
    "    return [x.name for x in local_device_protos]\n",
    "print(get_available_devices()) \n",
    "print(\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU'),\n",
       " PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "tf.config.list_physical_devices()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}